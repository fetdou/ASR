{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='color:purple'> Speech Recognition 語音識別 - 課程 05-  Speech Recognition - Language Translation   語音識別 -  語言翻譯   - Python Tutorial</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "人工智慧-深度學習\n",
    "\n",
    "A.I Tutorials \n",
    "Deep Learning 教程 \n",
    "\n",
    "''' \n",
    "\n",
    "    04 - 課程    - Speech Recognition 語音識別\n",
    "\n",
    "         課程 05 - Speech Recognition \n",
    "                            - Language Translation with Deep Learning\n",
    "                            - Recurrent Neural Networks (RNN) \n",
    "                                - Sequence-To-Sequence\n",
    "                                \n",
    "         課程 05 - 語音識別  - 語音識別 \n",
    "                            - 深度學習 進行 語音識別\n",
    "                            - 循環神經網絡 \n",
    "                                - Sequence-To-Sequence                     \n",
    "                                                \n",
    "'''  教程簡介 '''\n",
    "'''  語音識別 '''\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 資料準備\n",
    "\n",
    "    >> 你我互動學習園地\n",
    "        >> https://interactiveuandmetutorials.weebly.com/\n",
    "    \n",
    "    >> Python 程式語言 設計\n",
    "       >> https://pythonprogrammingtutorials.weebly.com/\n",
    "\n",
    ">> 檔案路徑目錄結構 -\n",
    "    ├──  課程 05-  Speech Recognition - Language Translation - 語音識別 - 語言翻譯\n",
    "    ├── data\\   \n",
    "         └── cmn-tw.txt\n",
    "         \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 磁碟區 C 中的磁碟是 OS\n",
      " 磁碟區序號:  68E3-32FD\n",
      "\n",
      " C:\\Users\\calvi\\OneDrive\\桌面\\課程SpeechRecogination\\課程5 的目錄\n",
      "\n",
      "2019/09/24  下午 03:54    <DIR>          .\n",
      "2019/09/24  下午 03:54    <DIR>          ..\n",
      "2019/09/20  上午 11:12    <DIR>          .ipynb_checkpoints\n",
      "2019/09/20  下午 01:07    <DIR>          data\n",
      "               0 個檔案               0 位元組\n",
      "               4 個目錄  44,599,590,912 位元組可用\n"
     ]
    }
   ],
   "source": [
    "ddir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\OneDrive\\桌面\\課程SpeechRecogination\\課程5\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 磁碟區 C 中的磁碟是 OS\n",
      " 磁碟區序號:  68E3-32FD\n",
      "\n",
      " C:\\Users\\calvi\\OneDrive\\桌面\\課程SpeechRecogination\\課程5\\data 的目錄\n",
      "\n",
      "2019/09/20  下午 01:07    <DIR>          .\n",
      "2019/09/20  下午 01:07    <DIR>          ..\n",
      "2018/10/03  下午 04:58         1,229,637 cmn-tw.txt\n",
      "               1 個檔案       1,229,637 位元組\n",
      "               2 個目錄  44,595,449,856 位元組可用\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\OneDrive\\桌面\\課程SpeechRecogination\\課程5\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 文字資料庫 - 範例\n",
    "    >> 將使用英語句子對應的中文語句翻譯的數據集\n",
    "    >> http://manythings.org/anki\n",
    "        >> Download \n",
    "            >> 下載數據集\n",
    "            \n",
    "            >> 下載的文件\n",
    "                >> cmn-eng.zip 簡中對應到英文\n",
    "                >> 為了更貼近學習的效果\n",
    "                >> 簡中 轉成了 繁中的版本 - cmn-tw.txt\n",
    "                    >> 從cmn-tw.txt上取得這個資料檔\n",
    "                >> 一個 字符級(character-level)的 - 序列到序列模型\n",
    "                >> 逐個字符地處理輸入，並逐個字符地產生輸出\n",
    "                >> 另一個選擇是一個字級(word-level)模型\n",
    "                >> 模型往往是機器翻譯更常見的\n",
    "                >> 在這的最後\n",
    "                    >> 會發現一些關於使用嵌入圖層 (embedding layers)\n",
    "                >> 模型轉換為字級模型的參考連結\n",
    " \n",
    " >> 下載 cmn-eng.zip 檔案\n",
    "     >> 新的子目錄  \\data\n",
    "        >> 下載的資料檔 複製到 \\data \\的目錄\n",
    "     \n",
    "     >> 目錄結構\n",
    "     \n",
    "     課程 05-  Speech Recognition - Language Translation - 語音識別 - 語言翻譯\n",
    "     data\\   \n",
    "     └── cmn-eng.txt\n",
    "     \n",
    "'''     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.wp.com/www.simplifiedpython.net/wp-content/uploads/2018/07/speech-recognition-python-1.png?w=682&ssl=1\" width=\"50%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/450/1*q_xg5WYxXL2HCHY5k8xVGg.gif\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1066/1*zaK7KBzFtPFodu4CyauDYw.gif\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 電腦翻譯\n",
    "    >> Computers Translate\n",
    "    >> 如何設計程式讓電腦來翻譯人類語言 - Human Language\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 簡單案例 -  西班牙語 到 英語- 逐字翻譯 - Word’s Translation\n",
    "    >> 只需用匹配的英語單詞替換每個西班牙語單詞 -  Matching\n",
    "    >> 忽略了 - 語法 Grammar 和 語境 context\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1499/1*aQn40w6gpzK53YZLSIfNoQ.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 改善方法 \n",
    "    >>  添加特定於語言的規則\n",
    "        >> 雙字短語翻譯為單個組\n",
    "        >> 名詞和形容詞 -  Nouns and Adjectives\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1393/1*GkdstlNoHkAO--aLw0_ZAA.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 使用 - 統計 Statistics 數據使電腦 更好地翻譯 -的更好\n",
    "    >> 訓練數據 -Training data\n",
    "    >> 基於概率和統計 statistics-based translation system \n",
    "        >> - rule-based systems\n",
    "        >> 而不是語法規則的模型開發了新的翻譯方法\n",
    "        \n",
    "    >> 電腦 - 可以使用平行語料庫 - Parallel Corpora\n",
    "        >> 來猜測如何將文本 Convert Text\n",
    "            >> 從一種語言轉換為另一種語言   \n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1926/1*GTArMUvb-aZARXxIiXuBXw.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 概率\n",
    "    >> Probabilities\n",
    "        >> 1. 將原始句子分成塊\n",
    "            >> 將句子分解成簡單的塊，每個塊都可以輕鬆翻譯\n",
    "        >> 2. 查找每個塊的所有可能的翻譯\n",
    "            >> 通過查找人類在訓練數據中翻譯相同塊詞的所有方法來翻譯每個塊\n",
    "        >> 3. 生成所有可能的句子並找到最可能的句子\n",
    "            >> 使用這些塊的每種可能組合來生成一堆可能的句子\n",
    "        \n",
    "    >> 統計機器翻譯 - 是一個巨大的里程碑    \n",
    "        >> 給他們足夠的- 訓練數據\n",
    "            >> 統計機器翻譯系統 - 比基於規則的系統執行得更好\n",
    "            >> Franz Josef Och 對這些想法 進行了改進\n",
    "                >> 並在21世紀初使用它們構建- 谷歌翻譯\n",
    "            >> 機器翻譯- 終於可以為全世界所用\n",
    "            \n",
    "    >> “Every time I fire a linguist, my accuracy goes up.”\n",
    "        — Frederick Jelinek      \n",
    "            \n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1301/1*zYf5RGSI4nlpm_yUUqbgjg.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1539/1*LNfHvNRdSTD4xCRvgI9t7A.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 通過以不同方式組合塊 來生成近2500種不同的 句子變體\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I love | to leave | at | the seaside | more tidy.\n",
    "I mean | to be on | to | the open space | most lovely.\n",
    "I like | to be |on | per the seaside | more lovely.\n",
    "I mean | to go | to | the open space | most tidy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 不同的單詞排序和不同的分塊方式\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I try | to run | at | the prettiest | open space.\n",
    "I want | to run | per | the more tidy | open space.\n",
    "I mean | to forget | at | the tidiest | beach.\n",
    "I try | to go | per | the more tidy | seaside"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 採取這種可能的翻譯\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I try | to leave | per | the most lovely | open space."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 遞歸神經網絡\n",
    "    >> Recurrent Neural Networks (RNN)\n",
    "    >> https://interactiveuandmetutorials.weebly.com/\n",
    "        >> https://interactiveuandmetutorials.weebly.com/244902987231070321473217832097-3550631243.html\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 神經網絡 - 根據房屋的屬性- 計算房屋 - 近似值\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1075/1*xvHk7SBUPBpvgnYIeljAzQ.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 神經網絡（或 RNN的簡稱）\n",
    "    >> 是其中神經網絡的先前狀態是輸入到下一個計算一個神經網絡的略微調整了的版本\n",
    "    >> 這意味著先前的計算- 會改變未來計算的結果！\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1075/1*9AfNUvfHmftLmdWydt_79w.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 編碼\n",
    "    >> Encodings\n",
    "    >> 提出一種編碼，每個可能的不同影像, 表示為一系列唯一數字\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2034/1*6kMMqLt4UBCrN7HtqNHMKw.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 編碼\n",
    "    >> Encodings\n",
    "    >> 提出一種編碼，將每個可能的不同句子表示為一系列唯一數字\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2116/1*oA4MrbJwmGmcSEVWupG7jQ.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 把句子送入- RNN\n",
    "    >> 一次一個字\n",
    "    >> 處理完最後一個單詞後的\n",
    "    >> 最終結果將是表示- 整個句子的值\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1096/1*hbpW6FRw7mrc607fwM9rKw.gif\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 翻譯吧！\n",
    "    >> 兩個RNN並將它們 端到端連接\n",
    "        >> 第一個RNN可以生成代表句子的編碼\n",
    "        >> 第二個RNN可以採用該編碼\n",
    "            >> 並且反向執行相同的邏輯以再次解碼原始句子\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1934/1*B24hDD3nGjfI4y3eNLNOgw.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 可以訓練第二個RNN 將句子解碼為西班牙語而不是英語\n",
    "    >> 可以使用我們的並行語料庫訓練數據 - parallel corpora training data \n",
    "        >> 來訓練它\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1931/1*fGzLsEwnEwFo2Wo9kzIvBw.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 構建序列 到序列翻譯系統 - 英文到中文 \n",
    "    >> Building your own Sequence-to-Sequence Translation System\n",
    "        >> English to Chinese \n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 程式範例"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 導入 相關 函式庫 \n",
    "   >> import relative Lib\n",
    "   \n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 流程如下：\n",
    "    >> 1. 將句子(sentence)轉換為3個 - Numpy數組\n",
    "        >> encoder_input_data\n",
    "        >> decoder_input_data\n",
    "        >> decoder_target_data\n",
    "          >> encoder_input_data\n",
    "                >> 包含英文句 子的one-hot向量化的 三維 形狀數組\n",
    "                >> num_pairs\n",
    "                >> max_english_sentence_length\n",
    "                >> num_english_characters\n",
    "          >> decoder_input_data\n",
    "                >> 包含中文句子的one-hot向量化的 三維形狀數組\n",
    "                >> num_pairs\n",
    "                >> max_chinese_sentence_length\n",
    "                >> num_chinese_characters\n",
    "          >> decoder_target_data\n",
    "          >> decoder_input_data 相同，但是 偏移了一個時間步長\n",
    "                >> decoder_target_data [:,t,：]\n",
    "                >> 將與 decoder_input_data [：,t+1,：]相同\n",
    "  \n",
    "    >> 2. 訓練一個基本的基於 LSTM的 - Seq2Seq模型\n",
    "        >> 來預測給出 \n",
    "            >> encoder_input_data\n",
    "            >> 和 decoder_input_data 的 decoder_target_data\n",
    "            >> 模型使用教師- 強制(teacher forcing)的手法\n",
    "    >> 3. 解碼一些句子\n",
    "        >> 以檢查模型是否正常工作\n",
    "            >> 將來自 encoder_input_data 的樣本\n",
    "                >> 轉換為\n",
    "                    >> 來自 decoder_target_data 的對應樣本\n",
    "\n",
    "\n",
    "    >> 網絡的架構構建\n",
    "        >> 可以參考以下的圖示:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.deeplearningessentials.science/img/seq2seq/encoderdecoder.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://4.bp.blogspot.com/-6DALk3-hPtA/WO04i5GgXLI/AAAAAAAABtc/2t9mYz4nQDg9jLoHdTkywDUfxIOFJfC_gCLcB/s640/Seq2SeqDiagram.gif\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 範例\n",
    "    >> 英語句子對應的中文語句翻譯的數據集\n",
    "    >> http://manythings.org/anki\n",
    "        >> Download \n",
    "            >> 下載數據集\n",
    "            \n",
    "            >> 下載的文件\n",
    "                >> 為cmn-eng.zip 簡中對應到英文\n",
    "                    >> 為了更貼近學習的效果\n",
    "                >> 簡中 轉成了 繁中的版本（cmn-tw.txt）\n",
    "                    >> 可以從cmn-tw.txt上取得這個資料檔\n",
    "                >> 實現一個 字符級(character-level)的 序列到序列模型\n",
    "                    >> 逐個字符地處理輸入，並逐個字符地產生輸出\n",
    "                >> 另一個選擇是一個字級(word-level)模型\n",
    "                    >> 模型往往是機器翻譯更常見的\n",
    "                >> 在這篇文章的最後\n",
    "                    >> 會發現一些關於使用嵌入圖層 (embedding layers)\n",
    "                >> 將模型轉換為字級模型的參考連結\n",
    " \n",
    " >> 檔案路徑目錄結構 -\n",
    "    ├──  課程 05-  Speech Recognition - Language Translation - 語音識別 - 語言翻譯\n",
    "    ├── data\\   \n",
    "         └── cmn-tw.txt\n",
    "\n",
    "    >> upzip 課程5 ->  cmn-tw.txt檔案\n",
    "     >> 新的子目錄  \\data\n",
    "        >> 下載的資料檔 複製到 \\data \\的目錄\n",
    "     \n",
    "'''                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\Anaconda36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "cmd": "Other",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "cls": "KernelMagics",
        "colors": "BasicMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "copy": "Other",
        "ddir": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "echo": "Other",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "BasicMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "profile": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "ren": "Other",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 磁碟區 C 中的磁碟是 OS\n",
      " 磁碟區序號:  68E3-32FD\n",
      "\n",
      " C:\\Users\\calvi\\OneDrive\\桌面\\課程SpeechRecogination\\課程5 的目錄\n",
      "\n",
      "2019/09/24  下午 04:40    <DIR>          .\n",
      "2019/09/24  下午 04:40    <DIR>          ..\n",
      "2019/09/20  上午 11:12    <DIR>          .ipynb_checkpoints\n",
      "2019/09/20  下午 01:07    <DIR>          data\n",
      "               0 個檔案               0 位元組\n",
      "               4 個目錄  43,200,503,808 位元組可用\n"
     ]
    }
   ],
   "source": [
    "%ddir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 1. \n",
    "    >> 引入相關的函數庫\n",
    "    \n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 置放訓練資料的目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# 訓練資料檔\n",
    "DATA_FILE = os.path.join(DATA_PATH, \"cmn-tw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 2. \n",
    "    >> 相關的參數\n",
    "    \n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64        # 訓練時的批次數量\n",
    "epochs = 100           # 訓練循環數\n",
    "latent_dim = 256       # 編碼後的潛在空間的維度 (dimensions of latent space)\n",
    "num_samples = 10000    # 用來訓練的樣本數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 3.\n",
    "    >> 資料預處理\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料向量化\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "input_characters = set() # 英文字符集\n",
    "target_characters = set() # 中文字符集\n",
    "\n",
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')\n",
    "\n",
    "# 逐行的讀取與處理\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 我們使用“tab”作為“開始序列[SOS]”字符或目標，“\\n”作為“結束序列[EOS]”字符。 <-- **重要\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "input_characters = sorted(list(input_characters)) # 全部輸入的字符集\n",
    "target_characters = sorted(list(target_characters)) # 全部目標字符集\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # 所有輸入字符的數量\n",
    "num_decoder_tokens = len(target_characters) # 所有輸目標字符的數量\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # 最長的輸入句子長度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # 最長的目標句子長度\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# 輸入字符的索引字典\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# 輸目標字符的索引字典\n",
    "\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# 包含英文句子的one-hot向量化的三維形狀數組（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 包含中文句子的one-hot向量化的三維形狀數組（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 \n",
    "# decoder_target_data [:, t，：]將與decoder_input_data [：，t + 1，：]相同\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# 把資料轉換成要用來訓練用的張量資料結構 <-- 重要\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 4.\n",
    "    >> 構建網絡架構\n",
    "    >> \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/44a4c60ee9446a14effc6057a16c9f12b61102b5/68747470733a2f2f692e696d6775722e636f6d2f4e7a766c4733582e706e67\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定義輸入的序列\n",
    "\n",
    "# 注意：因為輸入序列長度(timesteps)可變的情況，使用input_shape =（None，num_features）\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') # 需要取得LSTM的內部state, 因此設定\"return_state=True\"\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 我們拋棄掉`encoder_outputs`因為我們只需要LSTM cell的內部state參數\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "\n",
    "# 設定解碼器(decoder)\n",
    "# 注意：因為輸出序列的長度(timesteps)是變動的，使用input_shape =（None，num_features）\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "# 我們設定我們的解碼器回傳整個輸出的序列同時也回傳內部的states參數\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# 在訓練時我們不會使用這些回傳的states, 但是在預測時我們會用到這些states參數\n",
    "# **解碼器的初始狀態是使用編碼器的最後的狀態(states)**\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states) #我們使用`encoder_states`來做為初始值(initial state) <-- 重要\n",
    "\n",
    "# 接密集層(dense)來進行softmax運算每一個字符可能的機率\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義一個模型接收encoder_input_data` & `decoder_input_data`做為輸入而輸出`decoder_target_data`\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 打印出模型結構\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#print(encoder_outputs[0:5], state_h, state_c)\n",
    "\n",
    "print(encoder_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# from IPython.display import Image\n",
    "\n",
    "# 產生網絡拓撲圖\n",
    "\n",
    "# plot_model(model, to_file='seq2seq_graph.png')\n",
    "# Image('seq2seq_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 5.\n",
    "    >> 訓練模型\n",
    "    \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 設定模型超參數\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 開始訓練\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          vverbose=0,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 儲存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> STEP 6.\n",
    "    >>模型預測\n",
    "\n",
    "    >> 以下是預測階段的步驟:\n",
    "\n",
    "        >> 1. 對輸入進行編碼 (encode)\n",
    "            >>並取得 解碼器所需要的初始狀態 (initial decoder state)\n",
    "        >> 2. 以此初始狀態運行一步 解碼器\n",
    "            >>並以 開始序列 標記作為目標\n",
    "            >> 輸出 將是下一個目標標記\n",
    "        >> 3. 重複當前目標標記和當前狀態\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://4.bp.blogspot.com/-6DALk3-hPtA/WO04i5GgXLI/AAAAAAAABtc/2t9mYz4nQDg9jLoHdTkywDUfxIOFJfC_gCLcB/s640/Seq2SeqDiagram.gif\" width=\"75%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義要進行取樣的模型\n",
    "\n",
    "# 定義編碼器(encoder)的模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 定義解碼器LSTM cell的初始權重輸入\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # 解碼器(decoder)定義初始狀態(initial decoder state)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) #我們使用`decoder_states_inputs`來做為初始值(initial state)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義解碼器(decoder)的模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# 反向查找字符索引來將序列解碼為可讀的內容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "# 對序列進行解碼\n",
    "def decode_sequence(input_seq):\n",
    "    # 將輸入編碼成為state向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 產生長度為1的空白目標序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # 發佈特定的目標序列起始字符\"[SOS]\",在這個範例中是使用 \"\\t\"字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 對批次的序列進行抽樣迴圈\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 對符標抽樣\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 停止迴圈的條件: 到達最大的長度或是找到\"停止[EOS]\"字符,在這個範例中是使用 \"\\n\"字符\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目標序列(of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 更新 states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # 從訓練集中取出一個序列並試著解碼\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
