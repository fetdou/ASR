{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1 align='center' style='color:purple'> Speech Recognition 語音識別 - 課程 6-  Speech Recognition - Speech-Emotion-Analyzer  語音識別 - 聲音的感情分析器 - Python Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rezachu/emotion_recognition_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech-Emotion-Analyzer 聲音的感情分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech Emotion Analyzer\n",
    "The idea behind creating this project was to build a machine learning model that could detect emotions from the speech we have with each other all the time. Nowadays personalization is something that is needed in all the things we experience everyday.\n",
    "\n",
    "So why not have a emotion detector that will guage your emotions and in the future recommend you different things based on your mood. This can be used by multiple industries to offer different services like marketing company suggesting you to buy products based on your emotions, automotive industry can detect the persons emotions and adjust the speed of autonomous cars as required to avoid any collisions etc.\n",
    "\n",
    "Analyzing audio signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/master/images/joomla_speech_prosody.png\" width=\"100%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "人工智慧-深度學習\n",
    "\n",
    "A.I Tutorials \n",
    "Deep Learning 教程 \n",
    "\n",
    "''' \n",
    "\n",
    "    04 - 課程    - Speech Recognition 語音識別\n",
    "\n",
    "         課程 05 - Speech Recognition \n",
    "                            - Language Translation with Deep Learning\n",
    "                            - Recurrent Neural Networks (RNN) \n",
    "                                - Sequence-To-Sequence\n",
    "                                \n",
    "         課程 05 - 語音識別  - 語音識別 \n",
    "                            - 深度學習 進行 語音識別\n",
    "                            - 循環神經網絡 \n",
    "                                - Sequence-To-Sequence                     \n",
    "                                                \n",
    "'''  教程簡介 '''\n",
    "'''  語音識別 '''\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "人工智慧-深度學習\n",
    "\n",
    "A.I Tutorials \n",
    "Deep Learning 教程 \n",
    "\n",
    "''' \n",
    "\n",
    "    04 - 課程    - Speech Recognition 語音識別\n",
    "\n",
    "         課程 06 - Speech Recognition \n",
    "                            - Speech Emotion Recognition - Convolutional Neural Network \n",
    "                            - Build A Simple - Sound Emotion Analyzer\n",
    "                                \n",
    "         \n",
    "         課程 06 - 語音識別  - 語音情感識別器 - 卷積神經網絡 \n",
    "                                             \n",
    "                   程式範例  - 建立一個簡單的卷積神經網絡 - 聲音感情分析器                      \n",
    "                            \n",
    "                            \n",
    "'''  教程簡介 '''\n",
    "'''  語音識別 '''\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 資料準備\n",
    "\n",
    "    >> 你我互動學習園地\n",
    "        >> https://interactiveuandmetutorials.weebly.com/\n",
    "    \n",
    "    >> Python 程式語言 設計\n",
    "       >> https://pythonprogrammingtutorials.weebly.com/\n",
    "\n",
    "    \n",
    ">> 檔案路徑/\n",
    "    ├── 課程 6-  Speech Recognition - Speech-Emotion-Analyzer  語音識別 - 語音情感識別分析\n",
    "    ├── Audio/\n",
    "    ├        ├── output1.mp3 \n",
    "    ├        ├── output1.wav\n",
    "    ├        ├── \n",
    "    ├        \n",
    "    ├── Data/\n",
    "             ├── \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 數據集 Datasets - 情感言語和歌曲的Ryerson視聽數據庫（RAVDESS）\n",
    "    >> 使用 兩個數據集\n",
    "        >> RAVDESS - 語音 - speech和 歌曲 -  song\n",
    "        >> SAVEE\n",
    "\n",
    ">> Ryerson情感言語和歌曲視聽數據庫\n",
    "    >> https://zenodo.org/record/1188976#.XZFfpEYzZPZ\n",
    "\n",
    "    >> 12位- Actors 和 12位女演員 Actresses \n",
    "        >> 分別錄製了語音和歌曲- 版本\n",
    "        >> 情感 - Emotio\n",
    "            >> Disgust 厭惡\n",
    "            >> Neutral 中性\n",
    "            >> Surprised 詫異\n",
    "                >> 不包括在歌曲版本的數據\n",
    "        \n",
    "    >> 18號 - Actor\n",
    "        >> 沒有歌曲版本數據。\n",
    "    \n",
    "    >> RADVESS數據集中\n",
    "        >> 每個演員必須通過說和唱歌兩個句子\n",
    "            >> 每個句子兩次來表現8種情緒\n",
    "    \n",
    "        >> 沒有中立，厭惡和驚訝的情緒\n",
    "            >> 每個演員會針對每種情緒誘導4個樣本，因為這些情緒沒有演唱數據\n",
    "            >> 每個音頻波大約在4秒左右\n",
    "            >> 最先和最後一秒很可能會靜音\n",
    "    \n",
    "    >> 標準句子為：\n",
    "        1.孩子們在門邊說話 - Kids are talking by the door.\n",
    "        2.狗坐在門口       -Dogs are sitting by the door.\n",
    "        \n",
    "        \n",
    "      >> Male’s 憤怒只是增加了音量 Angry is simply increased in volume.​\n",
    "      >> Male’s Happy and Sad significant features were laughing and crying tone in the silenced period in the audio.\n",
    "          >> 快樂 和 悲傷 的重要特徵\n",
    "              >> 是在音頻的靜音期中的笑聲和哭聲\n",
    "      >> Female’s Happy, Angry and Sad are increased in volume.\n",
    "          >> 快樂，憤怒和悲傷增加了音量\n",
    "      >> Female’s Disgust would add vomiting sound inside\n",
    "          >> 厭惡感會在體內增加嘔吐聲\n",
    "    \n",
    ">> 排除\n",
    "    >> neutral\n",
    "    >> disgust\n",
    "    >> surprised\n",
    "        >> RAVDESS數據集進行了\n",
    "            >> 10類 CLASSES 識別\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"https://miro.medium.com/max/1280/1*V_rP9O0Sk5N_l8A_P_uB7w.png\" width=\"100%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"https://miro.medium.com/max/1545/1*qImiLykubf8uVqU8BJTUqg.png\" width=\"75%\"> "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 情緒類分佈 - 條形圖\n",
    "    >> Bar Chart - Emotional distribution \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1049/1*24IqeFa-tHgt5XvpMKH1Qw.png\" width=\"75%\"> "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 程式範例\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    ">> 語音識別 的方法 如圖\n",
    "    >> The approach used in this example for speaker identification is shown in the diagram\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"https://www.mathworks.com/help/examples/audio_wavelet/win64/xxSpeakerID01.png\" width=\"100%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\Anaconda36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stratified Shuffle Split\n",
    "\n",
    "X = combined_df.drop(['label'], axis=1)\n",
    "y = combined_df.label\n",
    "xxx = StratifiedShuffleSplit(1, test_size=0.2, random_state=12)\n",
    "for train_index, test_index in xxx.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male_negative    1152\n",
       "male_positive     768\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male_negative    288\n",
       "male_positive    192\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 259)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.wp.com/www.simplifiedpython.net/wp-content/uploads/2018/07/speech-recognition-python-1.png?w=682&ssl=1\" width=\"100%\"> "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
